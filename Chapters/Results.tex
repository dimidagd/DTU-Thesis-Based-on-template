\chapter{Results}\label{ch:Results}
This chapter presents the results of implementing the algorithms outlined in the previous chapters both in simulated and real data.

\section{Simulations}
Simulations provide a flexible and controllable environment to conduct preliminary assessment of an algorithm. In the context of tracking, the developed simulator allows one to control 
\begin{enumerate}[label=(\alph*)] 
	\item The number of targets around the own-ship
	\item Different motion models for each simulated target with arbitrary process noise (\Cref{sec:MotionModels}).
	\item Different sensor models for each target (\Cref{sec:ObservationModels}).
	\item Tun-able noise intensities for each sensor
	\item Different refresh rates for each simulated sensors
	\item Artificial outliers/clutter observations generation.
	\item Type of filter used, \ie Kalman Filter, Extended Kalman Filter, Particle Filter, and with or without Probabilistic Data Association extension.
\end{enumerate}

\section{Performance}


An important step in designing a Kalman Filter or any of its variants is that of \emph{tuning}. For a given application and a system design problem, the observation and process noise covariance matrices $R,Q$ must be chosen, so as to give an acceptable level of performance. What is more, it is important to be able to evaluate the \emph{constistency} of an estimator, \ie to be able to check to check during on-line for its possible divergence. Performance can be post-measured in terms of \emph{Mean Squared Error} (see \Cref{ssec:RMSE}) while consistency can be assessed real-time in terms of statistical \emph{NEES} (\Cref{ssec:NEES}) or \emph{NIS} (\Cref{ssec:NIS}) testing. The overall idea behind the consistency check is to explore different combinations of filter parameters that yield consistent estimations, \ie estimates that are neither too confident, nor too optimistic. At the same time a consistency check can act as an estimator's divergence criterion.

%The overall idea behind tuning is to evaluate different combinations of filter parameters that yield the optimal performance. If one has access to ground truth data, as derived either from an additional measurement system or from simulations, performance can be post-calculated based on \emph{MSE} (see \Cref{ssec:RMSE}) while \emph{consistency} can be assessed in a on-line statistical fashion based on the \emph{NEES} (see \cref{eq:NEES}). If ground truth data is not available additional methods exist such as the \emph{NIS} test (see \Cref{ssec:NIS}).


\begin{table}[H]
	\centering
	\caption{Different performance metrics}
	\label{tab:sensor_active}
	\begin{tabular}{lllll}
		\hline
		Abbv. & Name                                & Needs ground truth & Assessment   & Measures    \\ \hline
		MSE   & Mean Squared Error                  & Yes                & Off-line & Performance \\
		NEES  & Normalized Estimation Error Squared & Yes                & On-line     & Consistency \\
		NIS   & Normalized Innovation Squared       & No                 & On-line     & Consistency \\ \hline
	\end{tabular}
\end{table}

\subsection{Root Mean Square Error} \label{ssec:RMSE}
A common measure of an estimator's performance when ground truth history is available, is the  mean square error. 
\begin{framed}
Given $M$ estimates $\hat{x}^{i}_{1:T}$ and their matching ground truth $x^{0(i)}_{1:T}$, then

\begin{equation}\label{RMSE}
	RMSE(\hat{x}_k) = \sqrt{\frac{1}{T}\sum_{k=1}^{T}\frac{1}{M} \sum_{i=1}^{M} \norm{ {\hat{x}_k^{(i)} - x_k^{0(i)}}}^2}
\end{equation}
\end{framed}
The RMSE combines the variance and bias of the estimate,
\begin{equation}\label{key}
RMSE(\hat{x}_k)  = \text{var} \hat{x}_k + b_t^2
\end{equation}

Nevertheless, \emph{RMSE} requires a history of estimates and ground truth data, and as such is not a on-line performance index. What is more,  ground truth is not always available hence additional performance metrics had to be devised. 

\subsection{Normalized Estimation Error squared (NEES)}
\label{ssec:NEES}
A performance metric that is able to assess an estimate's quality on a per-sample basis, is the \emph{Normalized Estimation Error Squared} test. It belongs to the family of \emph{statistical tests}, since it incorporates the use of the covariance matrix $P_k$ in the calculations. Additionally, it is a on-line performance metric, and it can evaluate the consistency only at time sample $k$.

\begin{framed}
Given an estimate and its covariance matrix at time sample $k$, ($\hat{x}_k, P_k$) along with its matching ground truth $x_k$, then

\begin{equation}\label{eq:NEES}
NEES(\hat{x}_k) =  (\hat{x}_k - x_k)^T \, P_k^{-1} \, (\hat{x}_k - x_k) 
\end{equation}

\end{framed}

Under the \emph{Gaussanity} assumptions and given a correct tuning of the estimator, then the \emph{NEES} follows a chi-squared distribution of $n_x$  degrees of freedom

\begin{equation}\label{key}
NEES(\hat{x}_k) \sim \chi^2(n_x), \quad \hat{x}_k \in \realnumbers^{n_x}
\end{equation}

According to \cite{TargetTracking} the scalar index $NEES(\hat{x}_k)$ can characterize the filtering quality as,

\begin{description}
	\item[$< n_x$] The estimate is conservative, \ie, the ground truth value is close to the estimated value, but the confidence interval of $P_k$ are disproportionately large, thus the estimate is better than indicated from $P_k$.
	\item[$ \approx n_x $] The ground truth values are falling within the \emph{reasonable} confidence interval of $P_k$, hence the estimated covariance matches the observations.
	\item[$> n_x$] The estimate is too optimistic, \ie, the ground truth value $x_k$ lies outside the confidence intervals, hence the estimate is worse than indicated from $P_k$.
\end{description}
\subsection{Normalized Innovation Squared test (NIS)}\label{ssec:NIS}
Unfortunately, definite ground truth data $x_t$ is usually unavailable when recording real data, and as such an alternative test to NEES is defined. Instead of evaluating the consistency of the state estimate $\hat{x}_k$, one can evaluate the consistency of the \emph{predicted observation} $\predObserv_k$. The \emph{consistency of the observation estimate is highly correlated with the consistency of the state estimate} and thus it is a good index of abnormal estimator behavior \cite{Ivanov2014}. The \emph{Normalized Innovation error Squared} (NIS) defined in \Cref{eq:NIS} is an alternative to NEES that does not require ground truth information of the state.


\begin{framed}
	
\begin{equation}\label{eq:NIS}
NIS(\realobserv^j_k,\hat{x}_k) = ({\realobserv}^j_k - {\predObserv}^j_k)^T \, S_k^{-1} \, ({\realobserv}^j_k - {\predObserv}^j_k)
\end{equation}

where

\begin{description}
	\item[$ \realobserv^j_k $] is the $j$-th sensor's observation, arriving at sample time $k$.
	\item[$ \predObserv^j_k $] is the predicted observation at sample time $k$ for the \ith{j} sensor, \ie $ \predObserv_k =  h(\hat{x}_k)$.
	\item[$h^j(\cdot)$] is the observation model of the associated sensor which can be linearized around the latest estimate by $\matr{H}^j_k$ (see \cref{sec:KalmanFilters})
	\item[$S_k = H^j_k P_{k|k-1} {H^j_k}^T + R_k$] is the innovation covariance matrix (see \cref{sec:KalmanFilters}).
\end{description}
\end{framed}

As in the NEES test, the consistency check is based on the scalar value of $NIS(\observ^j_k, \hat{x}_k)$ :
\begin{description}
	\item[$< \alpha_{NIS}$] The estimator is overconfident, hence the process and/or measurement noise should be increased to accommodate for motion and sensor noise modeling errors.
	\item[$ \approx \alpha_{NIS} $] The observations are consistent, hence if the observation noise levels match the actual sensor noise levels, then the estimates are consistent as well.
	\item[$> \alpha_{NIS}$] The estimate is too optimistic, \ie, the ground truth value $x_k$ lies outside the confidence intervals, hence the estimate is worse than indicated from $P_k$.
\end{description}
In a similar fashion to Observation Gating, the NIS tests follows a chi-squared distribution and the value of $\alpha_{NIS}$ depends on the dimensonality of the observation vector $\observ \in \realnumbers^{n_y}$, and can be found by calculating the inverse of the chi-squared CDF with $n_y$ degrees of freedom (\Cref{eq:gammasol}), or by looking up at \Cref{fig:chisquaredtable} in \Cref{ssec:validationregion}.  A summary of different values for $\alpha_{NIS}$ is found in \Cref{tab:alphaNIS}.


\begin{table}[H]
	\centering
	\caption{Indicative threshold values for different sensor modalities. $NIS(\observ^j_k, \hat{x}_k) \approx \alpha_{NIS}$ allows one to infer predicted observation consistency.}
	\label{tab:alphaNIS}
	\begin{tabular}{llll}
		\hline
		Sensor                       & Observations               & $n_y$ & $\alpha_{NIS}$ \\ \hline
		W-band radar                 & bearing, range             & $ 2 $  & $9.2$              \\
		Camera                       & bearing                    & $ 1 $     & $6.6$              \\
		AIS (only geodetic location) & $\longi, \lat$                    & $ 2     $ & $ 9.2 $              \\
		AIS (with speed)             & $\longi, \lat, \boatspeed_{SOG},\heading_{COG}$                  & $ 4  $    & $ 13.2 $       \\
		Doppler radar                & bearing, range, range rate & $ 3  $    & $ 11.3  $             \\ \hline
	\end{tabular}
\end{table}


\subsection{Covariance matrix determinant}


Since the matrix $P_k$ encapsulating a target's state uncertainty is a covariance matrix, it is and remains by definition positive semi-definite. Hence one can calculate the determinant $$\det{P_k} = \prod_{i=1}^{n_x} \lambda_i$$ , where $\lambda_i$ are the eigenvalues of $P_k$ and $n_x$ is the dimensionality of the state $x$. Since the eigenvalues of the covariance matrix are proportional to the volume of an uncertainty ellipsoid, then their product represents a metric of the uncertainty volume, and can be used as a scalar index for the overall \textit{"amount of uncertainty"} in an estimate $\hat{x}$. It is important to mention that the eigenvalues are not scale invariant, hence the actual magnitude of $\det{P_k}$ depends on the relative scaling of the units in $P_k$. Having that in mind, the absolute value of $\det{P_k}$ is insignificant in the context of uncertainty monitoring. 

What the author found interesting is the relative fluctuations in $\det{P_k}$ over time samples $k$, as it indicates how the state uncertainty increases after a time update, or decreases after a sensor update step. This effect can be seen in 

In \Cref{fig:detp} one can see the effect of fusing asynchronous sensor observations on the determinant of the state covariance matrix $P_k$.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{detP.png}
	\caption{Determinant of the state covariance matrix $P_k$. Fusion of different measurement updates in an Extended Kalman Filter. One can observe that a simulated camera sensor provides new observations approximately every 10 samples($f_{\text{cam}} = \SI{1}{\Hz}$), whereas a simulated AIS sensors provides observations every 100 samples ($f_{\text{AIS}} = \SI{0.1}{\Hz}$). Each sensor reduces the uncertainty at a different level.}
	\label{fig:detp}
\end{figure}

