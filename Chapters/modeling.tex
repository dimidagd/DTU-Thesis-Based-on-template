\chapter{Modeling}
This sections is devoted to:

\begin{enumerate}
	\item Different state-space representations of a tracked target.
	\item Different motion models for a tracked target.
	\item Different sensors and their models	
\end{enumerate}

\section{State-space} \label{sec:SS}
Assuming the target's motion on a \emph{two-dimensional tracking plane}, then two different state-space descriptions can be used depending on the expression of the target's \emph{velocity vector}:

\begin{enumerate}
	\item Cartesian velocity model 
	
	$$\vect{x} = \left[x,y,\dot{x}, \dot{y},\omega\right]^{T} $$
		
	\item Polar velocity model 
	
	$$\vect{x} = \left[x,y,\boatspeed, \heading,\omega\right]^{T} $$
		
\end{enumerate}

where

\begin{subequations}
	\begin{align}
		\boatspeed &= \sqrt{\dot{x}^2 + \dot{y}^2}  \\
		\heading &= \arctan\left(\frac{\dot{y}}{\dot{x}}\right) \\
		\turnrate &= \dot{\heading}
	\end{align}
\end{subequations}

\begin{description}
	\item $(x,y)$ : the displacement relative to the local tracking coordinate system
	\item $\boatspeed$ : is the target's linear velocity magnitude relative to the tracking system. If the tracking system is fixed to the earth, then $\boatspeed = \text{SOG (Speed Over Ground)}$
	\item $\heading$ : is the target's velocity vector heading angle w.r.t the tracking system's x-axis. If the tracking system is an ENU system then simply $\heading_{COG} = \heading - \frac{\pi}{2}$.
	\item $\omega$ : the target's turn rate in the horizontal plane.
\end{description}






\section{Motion models} \label{sec:MotionModels}

As discussed in a previous section, when choosing a coordinate system for the target motion model, it is very convenient to assume the target's state on a the own-ship's two dimensional tracking plane.



In order to improve the quality the stability and the accuracy of the estimations, the ships are assumed to comply with dynamic motion models. The tracking problem therefore is that of estimating the model's parameters for a target, taking into consideration all available observations from various sensors.


The physical constraints and dynamics of the problem of tracking floating targets at sea, justify the use of a 2-D kinematic model. Such a model can capture with sufficient accuracy the kinematic behavior of a remote maneuvering vessel. In the context of tracking, targets are assumed to be point objects with negligible dimensions corresponding to the mean vessel position. Maneuvering ships exhibit very characteristic trajectories and as such their  motion can be sufficiently expressed for the purpose of tracking by second-order dynamical models.

While the motion models proposed in literature are numerous, most of them are maintaining a relatively low level of complexity. \emph{Linear motion} models assume a constant linear velocity(CV) or constant linear acceleration (CA) \cite{Schubert2008}. These models have the advantage that the state transition equations are linear, and thus Gaussian probability densities can be efficiently and optimally propagated as matrix multiplications. Unfortunately linear models are restricted to straight-line motions and are thus unable to model rotations, especially the yaw rate, into account. In general, a vessel follows slow parabolic-type maneuvers. If one introduces the angular speed of the target around its z-axis as well, the resulting models are referred to as \emph{curvilinear} models. These models can be further distinguished by the selected state variables which are assumed to be constant.

\subsection{Continuous constant turn rate and velocity (CTRV) model}


Starting with a time continuous model:

\begin{equation}
\begin{aligned}\dot{\vect{x}}(t) = f(\vect{x}(t)) &&,\vect{x} = \left[x,y,\boatspeed, \heading,\omega\right]^{T}  \end{aligned}
\end{equation}


then assuming that the linear velocity $\boatspeed$ and the turn rate $\omega$ remain constant  --- which is a fair assumption for a moving vessel within a short time period--- then the non-linear differential equations describing the motion are according to \cite{Schubert2008}

\begin{equation}\label{eq:CTRV_continuous}
f(\vect{x}) =\begin{bmatrix}\dot{x} \\ \dot{y} \\ \dot{\boatspeed} \\ \dot{\heading} \\ \dot{\omega}  \end{bmatrix}=  \begin{bmatrix}\boatspeed cos(\heading) \\ \boatspeed sin(\heading) \\ 0 \\ \omega \\ 0  \end{bmatrix}
\end{equation}

where the time dependency was omitted for clarity reasons. \Cref{eq:CTRV_continuous} is describing a \emph{constant turn rate and velocity} (\textbf{CTRV}) model since the linear and angular velocities accelerations are zero.

If the system is assumed to be driven by linear and rotational acceleration inputs $a \bra{\si{\meter\per\second^2}}$ and $\alpha \bra{\si{\radian\per\second^2}}$ respectively, then the coupled differential equations system in \eqref{eq:CTRV_continuous} becomes the \emph{nearly} constant turn rate and velocity model of \Cref{eq:CTRV_driven}.

\begin{equation}\label{eq:CTRV_driven}
f(\vect{x}) =\begin{bmatrix}\dot{x} \\ \dot{y} \\ \dot{\boatspeed} \\ \dot{\heading} \\ \dot{\omega}  \end{bmatrix}=  \begin{bmatrix}\boatspeed cos(\heading) \\ \boatspeed sin(\heading) \\ \lataccel \\ \omega \\ \angaccel  \end{bmatrix}
\end{equation}
%\todo{Fig speed from upsilon to V in figure?}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{2d_track_state_corr.png}
	\caption{Tracked target state vector top view, in the own-ship's tracking coordinate system}.
	\label{fig:state_vector}
\end{figure}

\subsection{Discretization of the CTRV motion model}

Since the system is simulated in discrete time, one can integrate the motion model differential equations of \Cref{eq:CTRV_continuous} within a sample time $T$ and obtain the discrete state-dependent transition function $g(\cdot)$ as well as the acceleration input matrix $G(\cdot)$.

By integrating on a sample time $T$,
\begin{equation}\label{eq:motionintegral}
\begin{aligned}
\vect{x}(t+T) = \vect{x}(t) + \int_{t}^{t+T}f(\vect{x}(\tau)d\tau &&\text{and}&& f(\vect{x(\tau)})=  \begin{bmatrix}\boatspeed cos(\heading) \\ \boatspeed sin(\heading) \\ \lataccel \\ \omega \\ \angaccel  \end{bmatrix}
\end{aligned}
\end{equation}

where 

\begin{equation}
\vect{x(\tau)} = \left[x,y,\boatspeed, \heading,\omega\right]^{T}
\end{equation}

is the state-space vector. 

The solution to the state transition of \cref{eq:motionintegral} comprises of two functions of the current state $g(\cdot), G(\cdot)$ as in \eqref{eq:solutionintegral}




\begin{align}\label{eq:solutionintegral}
\vect{x}(t+T) &= g\left(\vect{x}(t)\right)   + G(\vect{x}(t))\procnoise(t) \\
\textrm{or else} \nonumber\\
\vect{x}_{k+1} &= g\left(\vect{x}_k\right)   + G(\vect{x}_k)\procnoise_k
\end{align}

where the indices $k$ denote sample time instants and  $\procnoise = \bmat{\lataccel, \angaccel}^T$ comprises the process acceleration input.

where $g(\cdot)$ by direct integration, for $\vect{x}(t) =\left[x,y,\boatspeed, \heading,\omega\right]^{T} $ and $\lataccel=\angaccel=0$ is the \emph{state-dependent transition function}

\begin{equation}
g(\vect{x}(t)) = \vect{x}(t) +  \begin{bmatrix}
\frac{2\boatspeed}{\omega}sin(\omega T)cos(\heading + \frac{\omega T}{2})\\
-\frac{2\boatspeed}{\omega}sin(\omega T)cos(\heading + \frac{\omega T}{2})\\
0 \\
\omega T \\
0
\end{bmatrix} , \omega \neq 0
\end{equation}

or


\begin{equation}
g(\vect{x}(t)) = \vect{x}(t) +  \begin{bmatrix}
\boatspeed T cos(\heading)\\
\boatspeed T sin(\heading)\\
0 \\
0 \\
0
\end{bmatrix} ,  \omega = 0
\end{equation}

Likewise, the state depending acceleration input matrix $G(\cdot)$ can be found in \eqref{eq:inputMatrixState}, after solving the integral in \eqref{eq:motionintegral}, under the assumption that the input acceleration vector $\procnoise =\bmat{\lataccel, \angaccel}^T$ is constant within a sample time $T$, also known as zero-order-hold discretization \cite{6916122}.

\begin{equation}\label{eq:inputMatrixState}
G(\vect{x}) = \bmat{\frac{T^2}{2}\cos \heading &0\\ \frac{T^2}{2}\sin\heading &0 \\T &0\\0 &\frac{T^2}{2} \\0 &T}
\end{equation}
\subsection{Process noise assumptions}
Arriving directly to the direct discrete time form of \Cref{eq:solutionintegral} requires treating the model as \emph{deterministic}. Obtaining a \emph{stochastic} model requires casting a probabilistic description on the acceleration inputs $\lataccel$ and $\angaccel$. To further exemplify, $\lataccel$ and $\angaccel$ are treated as uncorrelated random variables with zero mean and standard deviations $\sigma_{\lataccel} \bra{\si{\meter\per\second^2}}$ and $\sigma_{\angaccel} \bra{\si{\radian\per\second^2}}$ respectively. The zero-mean assumption yields the assumed nearly constant speed and nearly constant turn rate (CTRV) behavior that is widely adopted as a motion model in filtering. Apparently some authors prefer the name \emph{constant turn models}, to distinguish them from the more complicated models used in aviation \cite{Li2003}.
%TODO Finish up with process noise input function as well

\section{Observation models} \label{sec:ObservationModels}

An \emph{observation model} or \emph{sensor model} $\observ_k = h(\vect{x_k}) + \measnoise_k$ expresses a sensor's observations $\observ_k$ at time instances $t_k$ given the state vector $\vect{x}$ .

 $\measnoise_k$ is assumed to be white Gaussian noise with covariance matrix $\measCov(k)$ that depends on the specific sensor. Depending on the selection of the state vector $\vect{x}$, the one can come up with a linear or nonlinear observation model.

The author is using the term \emph{observations} in the tracking context to refer to the post processed information derived from raw-sensor \emph{measurements}. In most cases the observations are a byproduct of multiple raw sensor measurements. For instance line of sight sensors such as a laser distance sensor, a radar, create dense point representations around the sensor's origin. These points can be grouped into geometric entities by various feature extraction algorithms, in order to create target \emph{observations}.  Likewise, a camera in the tracking context, instead of an imaging device, can be modeled as a relative bearing sensor, by pre-processing the raw image data through an ANN that extracts detected targets pixel locations within the frames. This abstraction level greatly simplifies the measurement models, since one can directly focus on the data-fusion and bypass the observation acquisition process, which at a certain level constitutes a separate and independent task. It is worth mentioning that the author in this thesis chose to focus on a \emph{medium level fusion approach} \cite{Luo2002}, which means that features(target's position) are fused to obtain features of better quality that can be employed by different tasks.
%\todo{Disambiguate different sensor fusion levels, provide a whole section?}



%TODO {insert radar pre and post processing clustering section, as well as ANN detection images}


\subsection{Radar model}

The output of the radar observation model is assumed to be the relative range and bearing measurements of the centroids of clustered raw radar measurements. The model is thus derived by calculating the relative position of a target to the sensor in polar form $\observ_{radar} = \begin{bmatrix}
\range,
\bearing
\end{bmatrix}^{T}$ , given the target's position $\begin{bmatrix}x_t, y_t\end{bmatrix}^{T}$ w.r.t the sensor's coordinate system.
%TODO rewrite based on known target and own-ship?

%\todo{Make corrections to figures from windows} Done
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{radar.png}
	\caption{The radar observation model, and the \{e\} (ENU) and \{t\} (tracking) coordinate systems.}
	\label{fig:radar_observation_model}
\end{figure}


If the radar's coordinate system, is for simplicity  chosen to coincide with the \emph{tracking coordinate system},  attached to the own-ships sea-following system. Then the tracking is with respect to the own-ship and the own-ships heading angle $\heading$ is not needed. It was later through the writing of this thesis, that such a system is appropriate for tracking remote targets under the given motion model assumptions. \Cref{ssec:movingreferenceframe} is devoted to further elaborate upon this issue.

%TODO DONEwrite about rotating reference frame
The observation model is thus,
\begin{equation}
\observ_{radar}(\vect{x}) = \begin{bmatrix}
\rho \\
\theta
\end{bmatrix} = \begin{bmatrix}
\sqrt{x^2 + y^2} \\
arctan(\frac{y}{x})
\end{bmatrix} + \measnoise_{radar}
\end{equation}


where the observation noise $\measnoise$ is assumed to be white, zero-mean Gaussian with covariance $\measCov_{radar} = \text{diag}\left[\variance{\rho}, \variance{\theta}\right] $ .



A reasonable sizing of the noise variances for simulation purposes can be
%\todo{write part on actual values selection}
\begin{description}
	\item $\sigma_\rho = \SI{3}{\meter}$
	\item $\sigma_\theta = \SI[parse-numbers=false]{1\frac{\pi}{180}}{\radian}  $
\end{description}

These noise intensities are a tuning parameter in a simulated test. Exaggerating on the simulated white noise intensities is a common approach when developing filtering algorithms, in order to assure robustness under non-linearity and non-Gaussianity effects when testing on real data.

\subsection{Converted measurements GPS model}


The relative GPS observation model calculates the target's relative position $\begin{bmatrix}x, y\end{bmatrix}^{T} $ w.r.t the own-ship's tracking coordinate system, which is directly a part of the target's state vector $\vect{x}$. 

Given the following:

\begin{enumerate}
\item The own-ship's geodetic coordinates $\begin{bmatrix}\lat_{0}, \longi_{0} \end{bmatrix}^{T}$
\item The own-ships sea-following heading angle from true north $\heading_{\textit{0}}$
\item The target's measured geodetic coordinates $\begin{bmatrix}\lat, \longi\end{bmatrix}^{T}$ which is usually available at sea through the Automatic Identification System.
\end{enumerate}

Then the observation model is,


\begin{equation}\label{eq:GPSobsvmode}
\observ_{GPS}(\vect{x},\begin{bmatrix}\lat_{0}, \longi_{0}, \heading_{\textit{0}} \end{bmatrix}^{T}) = \begin{bmatrix}
x \\
y
\end{bmatrix}_{GPS} = \underbrace{\begin{bmatrix}
x \\
y
\end{bmatrix}}_{\text{pre-converted to geodetic}} + \measnoise_{GPS}
\end{equation}

The fact that the geodetic coordinates origin and the heading angle of the own-ship is not included in the state-space representation leads to a \emph{linear observation model}.

The observations are pre-converted to \emph{tracking coordinates}, and the noise is added on the post-converted observations\eqref{eq:GPSobsvmode}.

The AIS observations $[\lat, \longi]^{T}$ can be transformed into tracking coordinates $[x,y]^{T}$ by using the transformations from \Cref{sec:transformations}.


\begin{equation}
\begin{bmatrix}
x \\
y
\end{bmatrix} = R_z(\frac{\pi}{2}-\heading_{\textit{0}}) \cdot F_1\left(F_2(\begin{bmatrix}
\lat \\
\longi
\end{bmatrix}),\begin{bmatrix}
\lat_0 \\ \longi_0
\end{bmatrix}\right)
\end{equation}

Where

\begin{itemize}
	\item $F_2\left(\begin{bmatrix}
	\lat \\
	\longi
	\end{bmatrix}\right)$ is the transformation from geodetic to ECEF coordinates (see \Cref{ssec:geodetic-to-ecef}).
	\item $F_1\left( \begin{bmatrix}
	X \\
	Y \\
	Z
	\end{bmatrix}_{ECEF},\begin{bmatrix}
	\lat_0 \\
	\longi_0
	\end{bmatrix}\right)$ is the transformation from ECEF to ENU, given the geodetic coordinates of the own-ship $\begin{bmatrix}
	\lat_0,
	\longi_0
	\end{bmatrix}^T$ as a reference point (see \Cref{ssec:ecef-to-enu}).
	\item $R_z(\theta)$ is the transformation from ENU to \emph{sea-following/tracking }coordinate system (see \Cref{ssec:transformation-from-enu-to-own-ship-tracking-coordinates}).
\end{itemize}

\subsection{AIS observation model}

An alternative method to using the aforementioned GPS model is to leverage the fact that the tracked targets are in the vicinity of the own-ship, and thus small differences in geodetic coordinates can be linearly mapped to ENU differences and the other way around \cite{Wellenhof1997}.



\begin{equation}
	\begin{bmatrix}
		\lat\\ \longi
		\end{bmatrix}_{\text{target}}=
		\begin{bmatrix}
		\lat_{0}\\ \longi_{0}
	\end{bmatrix}_{\text{own-ship}} + \begin{bmatrix}
	\delta\lat \\ \delta\longi
	\end{bmatrix} + \measnoise_{\textit{AIS}}
\end{equation}


By differentiating the model mentioned in the Coordinate systems section, one can confirm that for given own-ship geodetic coordinates $(\lat_0,\longi_0, h_0)_{\textit{own-ship}}$



\begin{equation}
\label{eq:ENUgeodifferences}
\begin{bmatrix}
dE\\dN\\dU
\end{bmatrix}=
\begin{bmatrix}
(N(\lat_0)+h_0)\cos\lat_0 &0 &0\\
0 &M(\lat_0) + h_0 &0\\
0 &0 &1
\end{bmatrix}
\begin{bmatrix}
d\longi\\d\lat\\dh
\end{bmatrix}
\end{equation}


Tracking is performed \emph{at sea level}, thus $h=0$, leading to


\begin{equation}
\begin{bmatrix}
d\longi\\ d\lat
\end{bmatrix}=
\matr{T}(\lat)^{-1}
\begin{bmatrix}
dE\\ dN
\end{bmatrix}
\end{equation}



Since the target's state is in the \emph{tracking/seakeeping} coordinate system, one needs to transform it to ENU.

If $\matr{R}_z(\heading)$ is the Euler's angle transformation for a rotation $\heading$ around the z-axis, then the transformation from \emph{sea-following/tracking} to \emph{ENU} coordinates, is

$$\matr{R}^{\textit{ENU}}_{\textit{sea-following}} $$

given the own-ship's \emph{heading from true-north} $\heading_{\textit{0}}$, then,


\begin{equation}
\matr{R}_{\textit{sea-following}}^{\textit{ENU}} = \matr{R}_{z}(\frac{\pi}{2}-\heading_{\textit{0}})
\end{equation}


\begin{equation}
\begin{bmatrix}
dE\\ dN
\end{bmatrix}=
R_{\textit{sea-following}}^{\textit{ENU}}
\begin{bmatrix}
x\\ y
\end{bmatrix}_{\textit{\{t\}}}
\end{equation}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{coords.png}
	\caption{Tracking frame and own-ship sensor frame}.
	\label{fig:track_vs_own_ship}
\end{figure}



and thus, by combining the above equations, one can solve for the observed $[\lat, \longi]^{T}$,

\begin{equation}
\begin{aligned}
\begin{bmatrix}
\lat\\ \longi
\end{bmatrix}&=
\begin{bmatrix}
\lat_{0}\\ \longi_{0}
\end{bmatrix} + {\matr{T}(\lat_{0})}^{-1}\begin{bmatrix}
dE\\ dN
\end{bmatrix} + \measnoise_{\textit{AIS}}=  \\
&\begin{bmatrix}
\lat_{0}\\ \longi_{0}
\end{bmatrix}_{\textit{own-ship}} + \matr{T}(\lat_{0})^{-1}
\matr{R}^{\textit{ENU}}_{\textit{sea-following}}(\heading_{\textit{0}})
\begin{bmatrix}
x_{\textit{t}}\\ y_{\textit{t}}
\end{bmatrix}_{\textit{wrt}_{{\textit{sea-following}}}}+\measnoise_{\textit{AIS}}
\end{aligned}
\end{equation}


Where

\begin{description}
	\item $\matr{T}(\lat)=
	\begin{bmatrix}
	N(\lat)\cos\lat &0\\
	0 &M(\lat)
	\end{bmatrix}
	$ is the transformation matrix from \emph{geodetic differences} to \emph{ENU differences} (see \Cref{eq:ENUgeodifferences}).
	\item $N(\lat) = \frac{\eqRadius}{\sqrt{1-e^2\sin^2\lat}}$ is the \emph{prime vertical radius}
	\item $M(\lat) = \frac{\eqRadius(1-e^2)}{(1-e^2\sin^2\lat)^{3/2}}$ is the \emph{meridional radius}
	\item $\eqRadius,\polarRadius$ are the equatorial radius(6378.1370 km) and the Polar radius(6356.7523 km) respectively, as derived from the \emph{WGS-84 ellipsoid model} \cite{Farrell2008} (see also \Cref{fig:earthellipradii}).
	\item $e = \sqrt{1 -\frac{\polarRadius^2}{\eqRadius^2}}$ is the ellipsoid's eccentricity.
	\item $h _0 \approx 0 $ since the own-ship is floating on the water surface.
	\item $\measnoise_{\textit{AIS}}$ is white Gaussian zero-mean noise with covariance matrix $\measCov_{\textit{AIS}}$ depending on the modeled noise intensity of the AIS source.
\end{description}

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{EarthEllipRadii}
	\caption{Three different radii as a function of Earth's latitude. $R$ is the geocentric radius; $M$ is the meridional radius; and $N$ is the prime vertical radius \cite{nima2000}.}
	\label{fig:earthellipradii}
\end{figure}


The above equation can be rewritten to the standard observation model formulation

\begin{equation}
\observ_{\textit{AIS}}(\vect{x},\begin{bmatrix}\lat_{0}, \longi_{0}, \heading_{\textit{0}} \end{bmatrix}^{T}) = \begin{bmatrix}
\lat\\ \longi
\end{bmatrix}_{\textit{AIS}} = \underbrace{\begin{bmatrix}
\lat_{0}\\ \longi_{0}
\end{bmatrix}}_{\textit{reference}} + \underbrace{\matr{H}\vect{x} }_{\textit{small differences model}} + \measnoise_{\textit{AIS}}
\end{equation}

, which is a \textbf{linear} observation model that is approximating the geoid with a \emph{spherical-linearization} about the own-ship's geodetic location. It is worth mentioning that as the tracking reference point moves from north to south, one would have to recalculate the transformation matrix of \Cref{eq:ENUgeodifferences}, to account for changes in earth's eccentricity. Nevertheless, in the time frame of a tracking scenario the eccentricity $e$ of the geoid remains almost constant throughout the tracking process, which allows one to assume that the observation function of the AIS $h_{\textit{AIS}}(\cdot)$ is a \emph{linear-time invariant} (LTI) system, as long as $\heading_{\textit{0}} = \const $. The last assumption is clearly violated in most tracking scenarios in which the own-ship is maneuvering and the heading angle $\heading$ is changing over time. This fact gave the author a lead on identifying that tracking targets with respect to the own-ship makes the $\text{AIS}$ observation model non-linear, and provided motivation for using a constant reference system.
\subsection{Camera observation model}


Assuming a camera sensor mounted on a mast with the camera z-axis aligned with the \emph{own-ship's body coordinate system}, then a top-view of the pinhole model is illustrated in \cref{fig:camera_pinhole}.



\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{camera2.png}
	\caption{Camera alignment with the \emph{tracking} frame}.
	\label{fig:camera_alignment_tracking}
\end{figure}




\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{camera_pinhole.png}
	\caption{Pinhole model top-view}.
	\label{fig:camera_pinhole}
\end{figure}


Then the observation model for the predicted pixel location of a target obtained from a deep neural-network classifier and localizer is found by projecting the position of a remote ship in the sea-following coordinate system $\vect{x}_{\text{wrt}_{\text{sea following}}}=\begin{bmatrix}x, y, 0, 1\end{bmatrix}^T$ to the finite camera frame\eqref{eq:projection_model}.

\begin{equation}\label{eq:projection_model}
\vec{\observ}_{cam} = P \cdot \vect{x}_{\text{wrt}_{\text{sea following}}}
\end{equation}


\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{camera_detections.pdf}
	\caption{Multiple ships detection using an Artificial Neural Network(ANN) approach. Courtesy of \cite{Blanke2020}.}
	\label{fig:multipleobjectdetections}
\end{figure}



 

The solution to \Cref{eq:projection_model} is
 

\begin{equation}
\observ_{\textit{cam}_{\textit{px}}}(\vect{x},z_{\textit{gyro}}) = \begin{bmatrix}u\\v\end{bmatrix}_{px}= \frac{\begin{bmatrix}-P_1-\\-P_2-\end{bmatrix}\vect{x}}{
	\begin{bmatrix}
	-P_3-
	\end{bmatrix}
	\vect{x}}+ \measnoise_{\text{camera}}
\end{equation}


Where $\matr{P}$ is the projection matrix and $P_{i}$ are it's rows

\begin{equation} \label{eq:projection}
P = \begin{bmatrix}-P_1- \\-P_2-\\-P_3- \end{bmatrix} = \matr{K}\matr{R_{\textit{own-ship}}^{\textit{camera}}}[\vect{I} | \matr{-C}]
\begin{bmatrix}
\matr{\matr{R}_{\textit{sea-following}}^{\textit{own-ship}}} &\vect{0}\\
\vect{0} &1
\end{bmatrix}
\end{equation}

Because of the denominator in \cref{eq:projection} this is a non-linear model, and a description of the matrices that comprise the projection matrix $P$ is given below:

\begin{description}
\item[$\matr{K}$]
is the \emph{intrinsic camera calibration} matrix

$$\matr{K} = \begin{bmatrix}f m_x &0 &p_x \\
0 &f m_y &p_y \\
0 &0 &z
\end{bmatrix} $$
sh s
where

\begin{description}
	\item[$f$] is the \emph{focal length} usually in mm, an intrinsic camera calibration parameter.
	\item[$m_x, m_y$] are the camera's sensor pixel density in $\frac{\text{number of pixels}}{mm}$ across the two different directions of the image plane.
	\item[$p_x,p_y$] correspond to the position of the principal point of the image in \emph{pixel coordinates} (see \cref{fig:camera_pinhole}).
	\item[$\measnoise_{\textit{camera}}$] is white Gaussian zero mean noise with variance related to the classifier accuracy, or any calibration errors in the extrinsic and intrinsic camera matrices.
	
\end{description}

\item[$\matr{R}_{\textit{own-ship}}^{\textit{camera}}$]
is the \emph{extrinsic camera rotation matrix }that describes the \emph{orientation of the camera coordinate system} w.r.t. the \emph{own-ship}.

\item[$\matr{C}$]
is the  \emph{extrinsic origin of the camera coordinate system} w.r.t. the \emph{own-ship}.


\item[$\matr{R}_{\textit{tracking}}^{\textit{own-ship}}$]
Is the transformation matrix that corresponds to the \emph{roll-pitch} rotation of the \emph{own-ship's body coordinate system} w.r.t the \emph{tracking} coordinate system.. Please note that \emph{the order of the rotations matters}.

\begin{equation}
R_{\textit{tracking}}^{\textit{own-ship}}=\left( R_{\textit{pitch}} R_{\textit{roll}}R_{\textit{yaw}}\right)^T
\end{equation}

Where $$R_{\textit{pitch}},R_{\textit{roll},R_{\textit{yaw}}}$$ are the \textbf{standard rotation matrices} \cite{LaVallea}for the Euler's angles $z_{\textit{gyro}}=(\roll,\pitch,\yaw)^T = (\textit{roll},\textit{pitch},\textit{yaw})^T$.


\begin{equation}
R_{\textit{roll}}= R_x(\roll) =
\begin{bmatrix}
1 &0 &0\\
0 &\cos\roll &-\sin \roll\\
0 &\sin \roll &\cos \roll
\end{bmatrix}
\end{equation}


,


\begin{equation}
R_{\textit{pitch}}= R_y(\pitch) =
\begin{bmatrix}
\cos\pitch &0 &\sin \pitch\\
0 &1 &0\\
-\sin \pitch &0 &\cos\pitch
\end{bmatrix}
\end{equation}
and
\begin{equation}\label{eq:rot_yaw}
R_{\textit{yaw}}= R_{z}(\yaw) =   \bmat{\cos\yaw &-\sin\yaw&0\\\sin\yaw &\cos \yaw&0\\0&0&1}
\end{equation}

A basic assumption at this point, is that the own-ship is able to measure the angles $(\roll,\pitch,\yaw)$ of the \emph{own-ship body coordinate system} w.r.t the \emph{tracking coordinate system}. These angles are usually available on a ship by using one or multiple \emph{gyro sensors}.

\end{description}

%given measured information $z_{gyro}$ about the rotational angle of the \emph{own-ship body coordinate system}, w.r.t the \emph{tracking coordinate system}.
%TODO DONEMake correctiosn to camera: done
 Using \emph{Artifical Neural Networks} for ship detection using camera images (see \cref{fig:multipleobjectdetections}), has been demonstrated in \cite{Blanke2020} and it is considered one of the \emph{State-of-the-Art} methods in image processing at the time of writing of this reading.

\subsection{Real sensors \& modeling}

The sensor types fused in this project are summarized in \cref{tab:sensor_inventory,tab:sensor_active}. A sensor can be classified as being

\begin{itemize}
\item\textbf{Proprioceptive}, measuring values internal to
	the system. Examples are own-ship's heading, speed, or COG
	\item\textbf{Exteroceptive} , obtaining information from the own-ship's environment (e.g bearing and distance to objects)
	\item\textbf{Passive}, using energy coming from the
	environment. 
	\item\textbf{Active}, emitting energy and then measuring the response.
\end{itemize}
 

When modeling the sensors, specific assumptions were made on the Gaussian white noise covariance matrixes $\measCov$. These parameters are subject to final tuning in order to match the ones from the real data. It is a common approach to start with rational assumptions for these values when still in the \emph{simulation design phase} and  experiment with different values by \emph{\quotes{tuning the knobs}} when designing the system that should work on the real data. 

Quite often, manufacturers provide noise in the form of maximum error on the real values without specifying whiteness or Gaussianity. The approach in this thesis when modeling real sensor noise, is to fit the maximum error value $e_{\text{max}}$ found in a sensors specification data-sheets, to a Gaussian distribution $\mathcal{N}(0,\variance{})$ by assuming that there is no bias error unless specified, and that $e_{\text{max}} = 3 \sigma$ where $\variance{}$ is the \emph{variance} of the fitted distribution $\mathcal{N}$.
\begin{table}[H]
		\centering
	\caption{Sensor inventory and observation types. DoF stands for \emph{Degrees of Freedom}}
	\label{tab:sensor_inventory}
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{llll}
		\toprule
		& \textbf{Sensor}         & \textbf{Raw Data}       & \textbf{Extracted Feature}                  \\ \hline
		&
		IMU &
		\begin{tabular}[c]{@{}l@{}}Accelerometers\\ Gyroscope\\ Magnetometer\end{tabular} &
		\multicolumn{1}{l}{6 DoF sensor pose (x,y,z, roll, pitch, yaw)} \\ \hline
		&
		GNSS(through AIS) &
		\begin{tabular}[c]{@{}l@{}}Latitude\\ Longitude\\ Altitude\\ Time\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}Position\\ Heading\\ Velocity\end{tabular} \\ \hline
		&GPS compass & \begin{tabular}[c]{@{}l@{}}Heading\\3 axis velocity\end{tabular}& \begin{tabular}[c]{@{}l@{}}Own-ship COG (Course Over Ground)\\ Own-ship SOG (Speed Over Ground)\end{tabular}  \\ \hline 
		& X-Band radar      & Bearing and range scans & Clustered objects 2 DoF relative pose (x,y) \T \\ \hline
		& Visible light camera & Image pixels            & Target class and relative bearing          \T \\ \bottomrule
	\end{tabular}%
}
\end{table}
\begin{table}[H]
		\centering
	\caption{Sensor inventory characteristics}
	\label{tab:sensor_active}
	\begin{tabular}{llll}
		\hline
		& \textbf{Sensor}         & \textbf{Active/Passive} & \textbf{Reference frame} \T \\ \hline
		& IMU                     & Passive                 & Proprioceptive                       \T \\ 
		& GNSS(through AIS)       & Passive                 & Proprioceptive \& Exteroceptive \\
		& X-Band Radar      & Active                  & Exteroceptive                         \\ 
		& Visible spectrum camera & Passive                 & Exteroceptive                         \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Radar}
The radar sensor used in the scope of this thesis is a \textbf{Navico Broadband}  \textbf{4G\tm X-Band} radar.
%TODO DONE Fill specifications table and picture}


\begin{table}[]
	\centering
	\caption{Radar modeled specifications}
	\label{tab:radar_specs}
	\begin{tabular}{ll}
		\hline
		\textbf{Sensor} & Radar \T \\ \hline
		\textbf{Modeled error standard deviation $\sigma$} & \T \\
		Range & $\SI{3}{\m}$ \\
		Bearing & $\SI{0.3}{\degree}$ \\ \hline
		\textbf{Parameters} & \T \\
		Field of View & $\SI{360}{\degree}$ \\
		Maximum Distance of View & $\approx \SI{35}{\nauticalmile}$ \\
		Field of View scan time & $\approx \SI{2}{\second}$ \\ \hline
	\end{tabular}
\end{table}
\subsubsection{AIS}

\begin{table}[H]
	\centering
	\caption{AIS modeled specifications}
	\label{tab:AIS_specs}
	\begin{tabular}{ll}
		\hline
		\textbf{Model} &  EM-TRAK R100 AIS RECEIVER \T \\ \hline
		Observations&\textbf{Standard deviation of error} $\mathbf{\sigma_e}$   \T \\ \hline
		${\longi ,\lat}$ & 6m (When converted to a tangential frame) \\
		${COG}$ & \SI{3}{\deg} \\
		${SOG}$ & \SI{0.6}{\knot} \T \\ \hline
		\textbf{Modeled update time} & 5-60s \T \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Cameras}

An array of cameras is available on-board the sensor rig, to ensure overlapping field of views. For the shake of completeness, the whole stack of sensors used throughout the whole autonomy project is described in \Cref{tab:stack} and can be seen in \Cref{fig:stack}. COVID-19 lock down implications denied the availability of actual camera images in this Thesis, but simulated target detections were included.


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{sensor_stack.jpeg}
	\caption{Full sensor stack mounted on a main mast.}
	\label{fig:stack}
\end{figure}

\begin{table}[H]
	\centering
	\caption{Sensor stack on the mast illustrated in \Cref{fig:stack}.}
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{cll}
	\hline
	Numbering in \cref{fig:stack} & Sensor                                                 & Used in the context of this thesis \T \\ \hline
	1 & Satellite compass                   & Used     \\
	2 & FMCW radar (X-band)                 & Used     \\
	3                                              & Color \& NIR \& IR (Heat sensitive) Industrial cameras & Not used                           \\
	4 & HikVision 180 degree camera         & Not used \\
	5 & HikVision Pan-tilt-zoom camera      & Not used \\
	6 & IP camera                           & Not used \\
	7 & 79GHz short-range radar           & Not used \\
	8 & HikVision IR (heat-sensitive) Camera & Not used \\ \hline
\end{tabular}%
}
	\label{tab:stack}
\end{table}

%TODO Fill about cameras?